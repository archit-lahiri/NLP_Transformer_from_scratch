{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7at-czKd1MIt"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from os.path import exists\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        pe.requires_grad = False\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:,:x.size(1)]\n"
      ],
      "metadata": {
        "id": "cY-eh3I9jgy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(q, k, v): #No masking or dropout option implementation, should implement later\n",
        "\n",
        "  scores = q.matmul(k.transpose(-2,-1))\n",
        "  scores /= math.sqrt(q.shape(-1))\n",
        "\n",
        "  scores = F.softmax(scores,dim = -1)\n",
        "  output = scores.matmul(v)\n",
        "\n",
        "  return output\n"
      ],
      "metadata": {
        "id": "qbbPMpt12K7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, n_heads, out_dim):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(out_dim, out_dim*3) #Creates matrix that we split into q,k,v later\n",
        "      # We do {out_dim * 3} cuz we are later gonna split it into 3 parts  (why nn.linear? Neural network basically just matrix multiplication)\n",
        "    self.n_heads = n_heads\n",
        "    self.out_dim = out_dim\n",
        "    self.out_dim_per_head = out_dim // n_heads\n",
        "    self.out = nn.Linear(out_dim,out_dim)\n",
        "\n",
        "  def split_heads(self, t): #t is a matrix\n",
        "    return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #what is x?\n",
        "    qkv = self.linear(x)\n",
        "    q = qkv[:,:, :self.out_dim]\n",
        "    k = qkv[:,:, self.out_dim:self.out_dim*2]\n",
        "    v = qkv[:,:, self.out_dim*2:]\n",
        "\n",
        "    q,k,v = [self.split_heads(t) for t in (q,k,v)]\n",
        "\n",
        "    q,k,v = [t.transpose(1,2) for t in (q,k,v)] # we switch row index 1 and 2 for the math (check blogpost)\n",
        "                                                #https://hyugen-ai.medium.com/transformers-in-pytorch-from-scratch-for-nlp-beginners-ff3b3d922ef7\n",
        "\n",
        "    scores = attention(q,k,v)\n",
        "    scores = scores.transpose(1,2).contiguous().view(scores.shape[0],-1,self.out_dim)\n",
        "\n",
        "    out = self.out(scores)\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "HDY8uVFq4MaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, inp_dim, inner_dim, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
        "    self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PFyBUZm04Ut3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadAttention(n_heads. inner_transformer_size, dropout)\n",
        "    self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
        "    self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
        "    self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
        "    self.dropout1 = nn.Dropout(dropout)\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x2 = self.norm1(x)\n",
        "    x = x + self.dropout1(self.mha(x2))\n",
        "    s2 = self.norm2(x)\n",
        "    x = x + self.dropout2(self.ff(x2))\n",
        "    return x"
      ],
      "metadata": {
        "id": "KCIEM1T8jVHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, n_code, n_heads, embed_size,\n",
        "               inner_ff_size, n_embeddings, seq_len, dropout = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
        "    self.pe = PositionalEmbedding(embed_size, seq_len)\n",
        "\n",
        "    encoders = []\n",
        "    for i in range(n_code):\n",
        "        encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
        "        self.encoders = nn.ModuleList(encoders)\n",
        "\n",
        "        #language model\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = x + self.pe(x)\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VqqNSIr9ja5n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}