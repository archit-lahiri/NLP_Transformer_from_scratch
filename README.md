# NLP_Transformer_from_scratch


Implementation of the transformer architecture heavily inspired by the 2016 paper: [Attention is all you need](https://arxiv.org/abs/1706.03762).

This implementation uses Pytorch to implement the major functions of the transformer including embedding, multi-headed attention, encoder etc.

The goal of making this code was to understand conceptually the working of the transformer architecture, how embeddings are processed, and how transformers have a context window. This was to give me the pre-requisite knowledge to write a review paper on chatbots, which at the time of writing this, is mostly made using large language models based off of transformers like OpenAI's GPT and Google's Bard. Ultimately this helped me understand the abilities and limitations of chatbots to a great degree.

To this end the practical application of this code using a dataset for translation or text generation has not been done yet, but will be in the near future. 






